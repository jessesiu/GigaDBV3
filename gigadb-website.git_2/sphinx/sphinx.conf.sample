#
# WARNING! While this sample file mentions all available options,
# it contains (very) short helper descriptions only. Please refer to
# doc/sphinx.html for details.
#
# Generated by Chef, do not hand edit
#


#############################################################################
## data source definition
#############################################################################

source gigadb
{
	# data source type. mandatory, no default value
	# known types are mysql, pgsql, mssql, xmlpipe, xmlpipe2, odbc
	type			= @type@

	#####################################################################
	## SQL settings (for 'mysql' and 'pgsql' types)
	#####################################################################

	# some straightforward parameters for SQL source types
	sql_host		= @sql_host@
	sql_user		= @sql_user@
	sql_pass		= @sql_pass@
	sql_db			= @sql_db@
	sql_port		= @sql_port@
}


# inherited source example
#
# all the parameters are copied from the parent source,
# and may then be overridden in this source definition
source gigadb_dataset : gigadb
{
	sql_ranged_throttle	= 100
	sql_query = WITH dtypes AS (SELECT dataset_id, array_to_string(array_agg(type_id), ' ') AS type_ids, array_to_string(array_agg(type.name), ' ') AS type_names FROM dataset_type, type WHERE type.id = dataset_type.type_id AND dataset_id >= $start AND dataset_id <= $end GROUP BY dataset_id), \
dprojects AS (SELECT dataset_id, array_to_string(array_agg(project.name), ' ') AS project_names, array_to_string(array_agg(project_id), ' ') AS project_ids FROM dataset_project, project WHERE project.id = dataset_project.project_id AND dataset_id >= $start AND dataset_id <= $end  GROUP BY dataset_id), \
dauthors AS (SELECT dataset_id, array_to_string(array_agg(author.name), ' ') AS author_names FROM dataset_author, author WHERE author.id = dataset_author.author_id AND dataset_id >= $start AND dataset_id <= $end  GROUP BY dataset_id), \
dcommonnames AS (SELECT dataset_id, array_to_string(array_agg(species_id), ' ') AS species_ids FROM dataset_sample left join sample ON dataset_sample.sample_id=sample.id WHERE dataset_id >= $start AND dataset_id <= $end GROUP BY dataset_id),  \
dexternal AS (SELECT dataset_id, array_to_string(array_agg(external_link_type_id), ' ') AS external_type_ids, array_to_string(array_agg(external_link_type.name), ' ') AS external_type_names FROM external_link, external_link_type WHERE external_link.external_link_type_id = external_link_type.id AND dataset_id >= $start AND dataset_id <= $end  GROUP BY dataset_id), \
dsubmitter AS (SELECT id, affiliation FROM gigadb_user), \
dlinks AS (SELECT dataset_id, array_to_string(array_agg(link.link), ' ') AS links FROM link WHERE dataset_id >= $start AND dataset_id <= $end GROUP BY dataset_id), \
dmanuscripts AS (SELECT dataset_id, array_to_string(array_agg(manuscript.identifier), ' ') AS manuscript_identifiers, array_to_string(array_agg(manuscript.pmid), ' ') AS manuscript_pmids FROM manuscript WHERE dataset_id >= $start AND dataset_id <= $end GROUP BY dataset_id) \
	SELECT dataset.id as id, \
	dataset.title as title, \
	dataset.description as description, \
	array_to_string(ARRAY['10.5524', dataset.identifier], '/') as identifier, \
	dataset.publication_date as dpublication_date, \
	dataset.modification_date as dmodification_date, \
	dtypes.type_names as dataset_type_names, \
	dtypes.type_ids as dataset_type_ids, \
	dprojects.project_ids as project_ids, \
	dprojects.project_names as project_names, \
	dauthors.author_names as author_names, \
	dcommonnames.species_ids as species_ids, \
	dexternal.external_type_ids as external_type_ids, \
	dexternal.external_type_names as external_type_names, \
	dmanuscripts.manuscript_identifiers as manuscript_identifiers, \
	dmanuscripts.manuscript_pmids as manuscript_pmids, \
	dlinks.links as links, \
	dsubmitter.affiliation as affiliation, \
	date_part('epoch',publication_date) as publication_date, \
	date_part('epoch',modification_date) as modification_date, \
        sample.code as sample \
FROM dataset \
	left join dtypes ON dataset.id=dtypes.dataset_id \
	left join dprojects ON dataset.id=dprojects.dataset_id \
	left join dauthors ON dataset.id=dauthors.dataset_id \
	left join dcommonnames ON dcommonnames.dataset_id=dataset.id \
	left join dexternal ON dexternal.dataset_id=dataset.id \
	left join dmanuscripts ON dmanuscripts.dataset_id=dataset.id \
	left join dlinks ON dlinks.dataset_id=dataset.id \
	left join dsubmitter ON dsubmitter.id=dataset.submitter_id \
        left join dataset_sample ON dataset_sample.dataset_id=dataset.id \
        left join sample ON dataset_sample.sample_id=sample.id \
WHERE dataset.id >= $start AND dataset.id <= $end;

	sql_query_range = SELECT MIN(id), MAX(id) FROM dataset

	sql_attr_uint = publication_date
	sql_attr_uint = modification_date
	sql_attr_multi = uint dataset_type_ids from field
	sql_attr_multi = uint project_ids from field
	sql_attr_multi = uint species_ids from field
	sql_attr_multi = uint external_type_ids from field

}

source gigadb_file : gigadb
{
	sql_ranged_throttle	= 100
	sql_query = SELECT file.id, file.dataset_id, file.name, file.location, file.extension, file.size, file.description, \
                    file.date_stamp as ddate_stamp, date_part('epoch',date_stamp) AS date_stamp, \
                    format_id, type_id, file_type.name AS type, file_format.name AS format, sample.s_attrs, \
                    species.common_name, species.genbank_name, species.scientific_name, species.tax_id \
                    FROM file \
                        LEFT JOIN dataset_sample ON file.dataset_id = dataset_sample.dataset_id \
                        LEFT JOIN sample ON dataset_sample.sample_id = sample.id \
                        LEFT JOIN species ON species.id = sample.species_id \
                        LEFT JOIN file_type ON file.type_id = file_type.id \
                        LEFT JOIN file_format ON file.format_id = file_format.id \
                    WHERE file.id >= $start AND file.id <= $end;

	sql_query_range = SELECT MIN(id), MAX(id) FROM file
	sql_attr_uint = date_stamp
	sql_attr_uint = format_id
	sql_attr_uint = type_id
	sql_attr_uint = size
}

#############################################################################
## index definition
#############################################################################

index dataset
{

	source			= gigadb_dataset
	path			= @dataset.path@
	docinfo			= extern
	mlock			= 0
	morphology		= none
	min_word_len		= 1
	min_infix_len =1
	charset_type		= utf-8

}

index file
{

	source			= gigadb_file
	path			= @file.path@
	docinfo			= extern
	mlock			= 0
	morphology		= none
	min_word_len	= 1
	min_infix_len   = 1
	charset_type	= utf-8

}


# inherited index example
#
# all the parameters are copied from the parent index,
# and may then be overridden in this index definition


# distributed index example
#
# this is a virtual index which can NOT be directly indexed,
# and only contains references to other local and/or remote indexes
# realtime index example
#

#############################################################################
## indexer settings
#############################################################################

indexer
{
	# memory limit, in bytes, kiloytes (16384K) or megabytes (256M)
	# optional, default is 32M, max is 2047M, recommended is 256M to 1024M
	mem_limit		= 32M

	# maximum IO calls per second (for I/O throttling)
	# optional, default is 0 (unlimited)
	#
	# max_iops		= 40


	# maximum IO call size, bytes (for I/O throttling)
	# optional, default is 0 (unlimited)
	#
	# max_iosize		= 1048576


	# maximum xmlpipe2 field length, bytes
	# optional, default is 2M
	#
	# max_xmlpipe2_field	= 4M


	# write buffer size, bytes
	# several (currently up to 4) buffers will be allocated
	# write buffers are allocated in addition to mem_limit
	# optional, default is 1M
	#
	# write_buffer		= 1M


	# maximum file field adaptive buffer size
	# optional, default is 8M, minimum is 1M
	#
	# max_file_field_buffer	= 32M
}

#############################################################################
## searchd settings
#############################################################################

searchd
{
	# [hostname:]port[:protocol], or /unix/socket/path to listen on
	# known protocols are 'sphinx' (SphinxAPI) and 'mysql41' (SphinxQL)
	#
	# multi-value, multiple listen points are allowed
	# optional, defaults are 9312:sphinx and 9306:mysql41, as below
	#
	# listen			= 127.0.0.1
	# listen			= 192.168.0.1:9312
	# listen			= 9312
	# listen			= /var/run/searchd.sock
	listen			= 9312
	#listen			= 9306:mysql41

	# log file, searchd run info is logged here
	# optional, default is 'searchd.log'
	log			= @log@

	# query log file, all search queries are logged here
	# optional, default is empty (do not log queries)
	query_log		= @query_log@

	# client read timeout, seconds
	# optional, default is 5
	read_timeout		= 5

	# request timeout, seconds
	# optional, default is 5 minutes
	client_timeout		= 300

	# maximum amount of children to fork (concurrent searches to run)
	# optional, default is 0 (unlimited)
	max_children		= 30

	# PID file, searchd process ID file name
	# mandatory
	pid_file		= @pid_file@

	# max amount of matches the daemon ever keeps in RAM, per-index
	# WARNING, THERE'S ALSO PER-QUERY LIMIT, SEE SetLimits() API CALL
	# default is 1000 (just like Google)
	max_matches		= 50000

	# seamless rotate, prevents rotate stalls if precaching huge datasets
	# optional, default is 1
	seamless_rotate		= 1

	# whether to forcibly preopen all indexes on startup
	# optional, default is 1 (preopen everything)
	preopen_indexes		= 1

	# whether to unlink .old index copies on succesful rotation.
	# optional, default is 1 (do unlink)
	unlink_old		= 1

	# attribute updates periodic flush timeout, seconds
	# updates will be automatically dumped to disk this frequently
	# optional, default is 0 (disable periodic flush)
	#
	# attr_flush_period	= 900


	# instance-wide ondisk_dict defaults (per-index value take precedence)
	# optional, default is 0 (precache all dictionaries in RAM)
	#
	# ondisk_dict_default	= 1


	# MVA updates pool size
	# shared between all instances of searchd, disables attr flushes!
	# optional, default size is 1M
	mva_updates_pool	= 1M

	# max allowed network packet size
	# limits both query packets from clients, and responses from agents
	# optional, default size is 8M
	max_packet_size		= 8M

	# crash log path
	# searchd will (try to) log crashed query to 'crash_log_path.PID' file
	# optional, default is empty (do not create crash logs)
	#
	# crash_log_path		= /var/log/crash


	# max allowed per-query filter count
	# optional, default is 256
	max_filters		= 10240

	# max allowed per-filter values count
	# optional, default is 4096
	max_filter_values	= 9999999


	# socket listen queue length
	# optional, default is 5
	#
	# listen_backlog		= 5


	# per-keyword read buffer size
	# optional, default is 256K
	#
	# read_buffer		= 256K


	# unhinted read size (currently used when reading hits)
	# optional, default is 32K
	#
	# read_unhinted		= 32K


	# max allowed per-batch query count (aka multi-query count)
	# optional, default is 32
	max_batch_queries	= 32


	# max common subtree document cache size, per-query
	# optional, default is 0 (disable subtree optimization)
	#
	# subtree_docs_cache	= 4M


	# max common subtree hit cache size, per-query
	# optional, default is 0 (disable subtree optimization)
	#
	# subtree_hits_cache	= 8M


	# multi-processing mode (MPM)
	# known values are none, fork, prefork, and threads
	# optional, default is fork
	#
	workers			= threads # for RT to work


	# max threads to create for searching local parts of a distributed index
	# optional, default is 0, which means disable multi-threaded searching
	# should work with all MPMs (ie. does NOT require workers=threads)
	#
	# dist_threads		= 4


	# binlog files path; use empty string to disable binlog
	# optional, default is build-time configured data directory
	#
	# binlog_path		= # disable logging
	binlog_path		= @binlog_path@


	# binlog flush/sync mode
	# 0 means flush and sync every second
	# 1 means flush and sync every transaction
	# 2 means flush every transaction, sync every second
	# optional, default is 2
	#
	# binlog_flush		= 2


	# binlog per-file size limit
	# optional, default is 128M, 0 means no limit
	#
	# binlog_max_log_size	= 256M


	# per-thread stack size, only affects workers=threads mode
	# optional, default is 64K
	#
	# thread_stack			= 128K


	# per-keyword expansion limit (for dict=keywords prefix searches)
	# optional, default is 0 (no limit)
	#
	# expansion_limit		= 1000


	# RT RAM chunks flush period
	# optional, default is 0 (no periodic flush)
	#
	# rt_flush_period		= 900


	# query log file format
	# optional, known values are plain and sphinxql, default is plain
	#
	# query_log_format		= sphinxql


	# version string returned to MySQL network protocol clients
	# optional, default is empty (use Sphinx version)
	#
	# mysql_version_string	= 5.0.37


	# trusted plugin directory
	# optional, default is empty (disable UDFs)
	#
	# plugin_dir			= /usr/local/sphinx/lib


	# default server-wide collation
	# optional, default is libc_ci
	#
	# collation_server		= utf8_general_ci


	# server-wide locale for libc based collations
	# optional, default is C
	#
	# collation_libc_locale	= ru_RU.UTF-8


	# threaded server watchdog (only used in workers=threads mode)
	# optional, values are 0 and 1, default is 1 (watchdog on)
	#
	# watchdog				= 1


	# SphinxQL compatibility mode (legacy columns and their names)
	# optional, default is 0 (SQL compliant syntax and result sets)
	#
	# compat_sphinxql_magics	= 1
}

# --eof--
